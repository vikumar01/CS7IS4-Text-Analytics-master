{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Analytics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YbW3w5fok2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Apr 12 10:53:10 2020\n",
        "\n",
        "@author: Piyush\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def textfeatures(features):\n",
        "\tphrase=[]\n",
        "\tfor feature in list(features):\n",
        "\t\twords = \"\"\n",
        "# \t\tprint(\"feature\",feature,type(feature))\n",
        "\t\tfor word in feature.split(\",\"):\n",
        "\t\t\tword = word.replace(\" \",\"\")\n",
        "\t\t\tword = word.replace(\"'\",\"\")\n",
        "\t\t\tword = word.replace(\"[\",\"\")\n",
        "\t\t\tword = word.replace(\"]\",\"\")\n",
        "\t\t\twords += word + \" \"\n",
        "# \t\t\tprint(\"word\",word)\n",
        "\t\tphrase.append(words)\n",
        "# \t\tbreak\n",
        "\treturn phrase\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\tdata = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/final_features.csv\").fillna(\"undefined\")\n",
        "\tlabels = data[\"author\"]\n",
        "\ttext = data[\"text\"]\n",
        "\t## text features\n",
        "\tnouns = data[\"nouns\"]\n",
        "\tnamed_ents = data[\"named_ents\"]\n",
        "\tverbs = data[\"verb\"]\n",
        "\tadjs = data[\"adj\"]\n",
        "\tnoun_phrases = data[\"noun_phrases\"]\n",
        "\n",
        "\t## count features\n",
        "\tcount_nouns = data[\"noun_tokens\"]\n",
        "\tcount_named_ents = data[\"entity_tokens\"]\n",
        "\tcount_verbs = data[\"verb_tokens\"]\n",
        "\tcount_adjs = data[\"adj_tokens\"]\n",
        "\tcount_noun_phrases = data[\"noun_phrases_tokens\"]\n",
        "\n",
        "\t## extra count features\n",
        "\tunique_words = data[\"num_unique_words\"]\n",
        "\tstopwords = data[\"num_stopwords\"]\n",
        "\tarticles = data[\"article\"]\n",
        "\tpunc = data[\"num_punctuations\"]\n",
        "\tmean_word_len = data[\"mean_word_len\"]\n",
        "\tnon_voc = data[\"n_non_voc\"]\n",
        "\n",
        "\tnoun_phrases = textfeatures(noun_phrases)\n",
        "\tnouns = textfeatures(nouns)\n",
        "\tverbs = textfeatures(verbs)\n",
        "\tnamed_ents = textfeatures(named_ents)\n",
        "\tadjs = textfeatures(adjs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM2Ifkg6p97A",
        "colab_type": "text"
      },
      "source": [
        "Vectorisation of features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqrrlLvMp3yy",
        "colab_type": "code",
        "outputId": "ee57a4d0-8219-4ff7-8989-ff34c961d25a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# TFIDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "# text = pd.DataFrame(tfidf.fit_transform(text).toarray())\n",
        "nouns = pd.DataFrame(tfidf.fit_transform(nouns).toarray())\n",
        "# verbs = pd.DataFrame(tfidf.fit_transform(verbs).toarray())\n",
        "# named_ents = pd.DataFrame(tfidf.fit_transform(named_ents).toarray())\n",
        "# adjs = pd.DataFrame(tfidf.fit_transform(adjs).toarray())\n",
        "# noun_phrases = pd.DataFrame(tfidf.fit_transform(noun_phrases).toarray())\n",
        "\n",
        "## ordinal encoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "labels = pd.DataFrame(OrdinalEncoder().fit_transform(labels.to_numpy().reshape(-1,1)))\n",
        "print(\"encoded\")\n",
        "\n",
        "## joining\n",
        "# X = pd.concat([count_nouns,count_named_ents,count_verbs,count_adjs,count_noun_phrases,\n",
        "#               unique_words,stopwords,articles,punc,mean_word_len,non_voc,\n",
        "#               nouns,named_ents,verbs])\n",
        "# print(\"joined\",X.shape)\n",
        "\n",
        "## de allocating memory\n",
        "# nouns = named_ents = verbs = adjs = count_nouns = count_named_ents = count_verbs = count_adjs = count_noun_phrases = unique_words = stopwords = articles = punc = mean_word_len = non_voc = 0\n",
        "print(\"de allocation\")\n",
        "\n",
        "## train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "steps = 25000\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(nouns, labels, test_size=0.1,shuffle = True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoded\n",
            "de allocation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZpYL8WFp1kj",
        "colab_type": "text"
      },
      "source": [
        "Training a model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiMwjmmdt6AZ",
        "colab_type": "text"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skBeMBbOp0-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## training a model\n",
        "# from sklearn.svm import SVC\n",
        "# svm = SVC(kernel = \"linear\",C=0.025, random_state = 13)\n",
        "# svm.fit(X_train,Y_train)\n",
        "# y_pred = svm.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTCzE6qDt8VA",
        "colab_type": "text"
      },
      "source": [
        "Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUrN6qcDWJLJ",
        "colab_type": "code",
        "outputId": "155cd567-092a-49d4-c929-def1b7c84fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# from sklearn.linear_model import RidgeClassifier\n",
        "# clf = RidgeClassifier()\n",
        "# clf.fit(X_train,Y_train)\n",
        "# Y_predict = clf.predict(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_ridge.py:940: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw4UodzpCB6o",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "LBGM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoOQgQwWCCe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "mdl = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=63, max_depth=-1, learning_rate=0.1, n_estimators=100)\n",
        "mdl.fit(X_train, Y_train.values.ravel())\n",
        "Y_predict = mdl.predict(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCxpXqc7WiPf",
        "colab_type": "code",
        "outputId": "050860a7-a8ed-4ca9-8663-b563eadf92db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "## confusion matrix\n",
        "\n",
        "from sklearn.metrics import mean_squared_error,confusion_matrix,accuracy_score,precision_recall_fscore_support\n",
        "confMatrix = confusion_matrix(Y_test,Y_predict)\n",
        "accuracy_noun = accuracy_score(Y_test,Y_predict)\n",
        "precision_recall_fscore = precision_recall_fscore_support(Y_test,Y_predict)\n",
        "\n",
        "print(\"[nouns] Accurracy {} Precision recall {} confusion matrix {} \".format(accuracy_noun,precision_recall_fscore,confMatrix))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nouns] Accurracy 0.503216583273767 Precision recall (array([0.53050109, 0.61466165, 0.56047198, 0.31940299]), array([0.63001294, 0.59025271, 0.61889251, 0.24970828]), array([0.57599054, 0.60220994, 0.58823529, 0.28028815]), array([773, 554, 614, 857])) confusion matrix [[487  30  61 195]\n",
            " [ 73 327  23 131]\n",
            " [ 80  24 380 130]\n",
            " [278 151 214 214]] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDnaO3zHWqRb",
        "colab_type": "text"
      },
      "source": [
        "Pure test SET\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOly26aJWolA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## testing previous set\n",
        "from sklearn.metrics import mean_squared_error,confusion_matrix,accuracy_score,precision_recall_fscore_support\n",
        "Y_test_predict = clf.predict(nouns.iloc[steps:,:])\n",
        "Y_test_final = labels.iloc[steps:,:]\n",
        "\n",
        "confMatrix = confusion_matrix(Y_test_final,Y_test_predict)\n",
        "accuracy_noun = accuracy_score(Y_test_final,Y_test_predict)\n",
        "precision_recall_fscore = precision_recall_fscore_support(Y_test_final,Y_test_predict)\n",
        "\n",
        "\n",
        "print(\"[nouns][TEST] Accurracy {} Precision recall {} confusion matrix {} \".format(accuracy_noun,precision_recall_fscore,confMatrix))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}